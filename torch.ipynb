{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from torchvision.transforms import Compose,GaussianBlur,RandomAutocontrast\n",
    "from torchvision.models import resnet18,ResNet18_Weights\n",
    "import os\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import RichModelSummary,EarlyStopping\n",
    "import gradio as gr\n",
    "from collections import OrderedDict as Odict\n",
    "import numpy as np\n",
    "from tqdm.contrib.concurrent import process_map,thread_map\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError,MeanAbsoluteError,MeanSquaredError\n",
    "from torchmetrics.classification import MulticlassAveragePrecision\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Literal\n",
    "import gc\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1fdf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset/MildDemented', './dataset/VeryMildDemented', './dataset/NonDemented', './dataset/ModerateDemented']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading files in ./dataset/ModerateDemented: 100%|██████████| 52/52 [00:00<00:00, 180699.10it/s]\n",
      "reading files in ./dataset/MildDemented: 100%|██████████| 717/717 [00:00<00:00, 13354.10it/s]\n",
      "reading files in ./dataset/VeryMildDemented: 100%|██████████| 1790/1790 [00:00<00:00, 14644.23it/s]\n",
      "reading files in ./dataset/NonDemented: 100%|██████████| 2560/2560 [00:00<00:00, 9665.00it/s] \n",
      "reading folders: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]\n",
      "adding to dataset: 100%|██████████| 5119/5119 [00:02<00:00, 1982.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5119, 3, 208, 176]), torch.Size([5119, 4]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AlzDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        super().__init__()\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index],self.y[index]\n",
    "\n",
    "\n",
    "class LitAlzData(L.LightningDataModule):\n",
    "    def __init__(self,root='./dataset',transform=None,batch_size=128):\n",
    "        super().__init__()\n",
    "        self.root=root\n",
    "        self.dirs=[]\n",
    "        self.f=True\n",
    "        self.class_map=[]\n",
    "        self.x=[]\n",
    "        self.y=[]\n",
    "        self.total=0\n",
    "        self.transform=transform\n",
    "        self.workers=os.cpu_count()\n",
    "        self.batch_size=batch_size\n",
    "    def prepare_data(self):\n",
    "        self.data=Odict()\n",
    "        for root,dirs,files in os.walk(self.root):\n",
    "            if len(dirs):\n",
    "                if self.f:\n",
    "                    self.dirs=[os.path.join(root,i) for i in dirs]\n",
    "                    self.f=False\n",
    "                    continue\n",
    "                else:\n",
    "                    print('prepare_data has aldready been run')\n",
    "                    return\n",
    "            self.data[root.split('/')[-1]]={'names':[files]}\n",
    "        self.dirs.reverse()\n",
    "        print(self.dirs)\n",
    "        res=process_map(self.read_dir,\n",
    "                        self.dirs,\n",
    "                        tqdm_class=tqdm,\n",
    "                        total=len(self.dirs),\n",
    "                        desc='reading folders',\n",
    "                        )\n",
    "        for i in enumerate(self.data.keys()):\n",
    "            self.data[i[1]]['images']=res[i[0]]\n",
    "            self.total+=len(res[i[0]])\n",
    "        self.class_map={i[1]:i[0] for i in enumerate(self.data.keys())}\n",
    "        with tqdm(range(self.total),desc='adding to dataset') as pbar:\n",
    "            for i in self.data.keys():\n",
    "                for j in self.data[i]['images']:\n",
    "                    if self.transform:\n",
    "                        self.x.append(self.transform(j))\n",
    "                    else:\n",
    "                        self.x.append(j)\n",
    "                    self.y.append(self.class_map[i])\n",
    "                    pbar.update(1)\n",
    "        self.x=torch.tensor(np.array(self.x)).repeat_interleave(3,1).float()\n",
    "        self.y=torch.tensor(np.array(self.y))\n",
    "        self.y=F.one_hot(self.y,4).float()\n",
    "                \n",
    "    def read_dir(self,root):\n",
    "        # np.array(list(repeat(root+\"/\",times=len(os.listdir(root)))))+np.array(os.listdir(root))\n",
    "        files=np.array([f for f in os.listdir(root) if os.path.isfile(os.path.join(root, f))])\n",
    "        prefixes=np.repeat(root+\"/\",len(files))\n",
    "        paths=list(prefixes+files)        \n",
    "        # paths=root+\"/\"+np.array(os.listdir(root))\n",
    "        imgs=thread_map(self.read_img,\n",
    "                        paths,\n",
    "                        tqdm_class=tqdm,\n",
    "                        desc=f'reading files in {root}',\n",
    "                        )\n",
    "        return imgs\n",
    "\n",
    "    def read_img(self,file):\n",
    "        return torchvision.io.read_image(file)\n",
    "    \n",
    "    def setup(self,stage='fit'):\n",
    "        self.train_ds,self.val_ds,self.test_ds=random_split(\n",
    "            AlzDataset(self.x,self.y),\n",
    "            [0.8,0.1,0.1], \n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,shuffle=True,num_workers=self.workers,batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,shuffle=False,num_workers=self.workers,batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds,shuffle=False,num_workers=self.workers,batch_size=self.batch_size)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return self.val_dataloader()\n",
    "alzdata=LitAlzData(transform=Compose((RandomAutocontrast(),GaussianBlur(5,(0.01,2)))))\n",
    "alzdata.prepare_data()\n",
    "alzdata.x.shape,alzdata.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503ba057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitClasssifer(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LitClasssifer(L.LightningModule):\n",
    "    def __init__(self,classes=4):\n",
    "        super().__init__()\n",
    "        self.classes=classes\n",
    "        self.metrics=[MulticlassAveragePrecision(4),MeanAbsolutePercentageError(),MeanAbsoluteError(),MeanSquaredError()]\n",
    "        self.model=resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.model.fc=nn.Sequential(nn.Linear(512,classes))\n",
    "        self.loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.model.parameters(),lr=1e-4)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def log_metrics(self,stage:Literal['fit','val','test'],outputs,y,loss):\n",
    "        self.log(f'{stage}_loss',loss,logger=True,prog_bar=True)\n",
    "        for i in enumerate(self.metrics):\n",
    "            if i[0]==0:\n",
    "                i=i[1]\n",
    "                self.log(f'{stage}_{i._get_name()}',i.to(outputs)(outputs,y.argmax(1)),logger=True,prog_bar=True)\n",
    "            else:\n",
    "                i=i[1]\n",
    "                self.log(f'{stage}_{i._get_name()}',i.to(outputs)(outputs,y),logger=True,prog_bar=True)\n",
    "\n",
    "    def perform_step(self,batch,stage:Literal['fit','val','test']):\n",
    "        x,y=batch\n",
    "        outputs=self.model(x)\n",
    "        loss=self.loss_fn(outputs,y)\n",
    "        self.log_metrics(stage,outputs,y,loss)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        return self.perform_step(batch,'fit')\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        self.perform_step(batch,'val')\n",
    "    \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        self.perform_step(batch,'test')\n",
    "        \n",
    "    def predict_step(self,batch):\n",
    "        x,_=batch\n",
    "        outputs=self.model(x)\n",
    "        return outputs\n",
    "model=LitClasssifer()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4702a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer=L.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[RichModelSummary(2),EarlyStopping(monitor='val_loss',min_delta=0.1)],\n",
    "    log_every_n_steps=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c1de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data has aldready been run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ model         │ ResNet            │ 11.2 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ model.conv1   │ Conv2d            │  9.4 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ model.bn1     │ BatchNorm2d       │    128 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ model.relu    │ ReLU              │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ model.maxpool │ MaxPool2d         │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ model.layer1  │ Sequential        │  147 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ model.layer2  │ Sequential        │  525 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ model.layer3  │ Sequential        │  2.1 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ model.layer4  │ Sequential        │  8.4 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ model.avgpool │ AdaptiveAvgPool2d │      0 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ model.fc      │ Sequential        │  2.1 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>│ loss_fn       │ CrossEntropyLoss  │      0 │ train │\n",
       "└────┴───────────────┴───────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ model         │ ResNet            │ 11.2 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ model.conv1   │ Conv2d            │  9.4 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ model.bn1     │ BatchNorm2d       │    128 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ model.relu    │ ReLU              │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ model.maxpool │ MaxPool2d         │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ model.layer1  │ Sequential        │  147 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ model.layer2  │ Sequential        │  525 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ model.layer3  │ Sequential        │  2.1 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ model.layer4  │ Sequential        │  8.4 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ model.avgpool │ AdaptiveAvgPool2d │      0 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ model.fc      │ Sequential        │  2.1 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ loss_fn       │ CrossEntropyLoss  │      0 │ train │\n",
       "└────┴───────────────┴───────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 11.2 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 11.2 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 44                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 70                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 11.2 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 11.2 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 44                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 70                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78027cb17af4c39be1e632407298dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc2fd8487f84409adf6a786e97dd177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79621c24045424788f6f402f2d0c651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428d306125304391ae8ce4fb45ec7c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd3b528d2c2412d82bb144e56f77b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109cac489ce246ecbb99221d6518e68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3996391c1050404d8c626ca2d9fab227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c20aa66be8467996a032eb41450640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaa4510904d4c919b8c206f64c86659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    datamodule=alzdata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16292ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data has aldready been run\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff415a8715ee49e08a5b3c93b35e4875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">         Validate metric         </span>┃<span style=\"font-weight: bold\">          DataLoader 0           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_MeanAbsoluteError      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       2.7283077239990234        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> val_MeanAbsolutePercentageError </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1621634.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_MeanSquaredError       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        9.29572868347168         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> val_MulticlassAveragePrecision  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.9788755178451538        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            val_loss             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.168421670794487        </span>│\n",
       "└─────────────────────────────────┴─────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m        Validate metric        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m         DataLoader 0          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_MeanAbsoluteError     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      2.7283077239990234       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mval_MeanAbsolutePercentageError\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1621634.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_MeanSquaredError      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       9.29572868347168        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mval_MulticlassAveragePrecision \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.9788755178451538       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           val_loss            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.168421670794487       \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────────┴─────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare_data has aldready been run\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7aa51b980454adbbc4499c91a6e3a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">           Test metric            </span>┃<span style=\"font-weight: bold\">           DataLoader 0           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_MeanAbsoluteError      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        2.7420859336853027        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_MeanAbsolutePercentageError </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           1629736.875            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_MeanSquaredError       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        9.525773048400879         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_MulticlassAveragePrecision  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.9932016730308533        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            test_loss             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       0.12214943021535873        </span>│\n",
       "└──────────────────────────────────┴──────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m          Test metric           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m          DataLoader 0          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_MeanAbsoluteError     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       2.7420859336853027       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_MeanAbsolutePercentageError\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          1629736.875           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_MeanSquaredError      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       9.525773048400879        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_MulticlassAveragePrecision \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.9932016730308533       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           test_loss            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      0.12214943021535873       \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────────────────────────┴──────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.12214943021535873,\n",
       "  'test_MulticlassAveragePrecision': 0.9932016730308533,\n",
       "  'test_MeanAbsolutePercentageError': 1629736.875,\n",
       "  'test_MeanAbsoluteError': 2.7420859336853027,\n",
       "  'test_MeanSquaredError': 9.525773048400879}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(\n",
    "    model,\n",
    "    datamodule=alzdata\n",
    ")\n",
    "trainer.test(\n",
    "    model,\n",
    "    datamodule=alzdata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6679cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
